#!/bin/bash
echo "ğŸš€ [Pre-start] æ­£åœ¨æ¸…ç†å¹¶åˆå§‹åŒ– GPU ç¯å¢ƒ..."

# 1. å¼ºåˆ¶é‡Šæ”¾æ˜¾å­˜å¹½çµè¿›ç¨‹
fuser -k /dev/nvidia0

# 2. ç¯å¢ƒå˜é‡å¼ºåˆ¶åŠ è½½ (å®Œå…¨å¤åˆ»æ˜¨æ—¥æˆåŠŸé…ç½®)
export OLLAMA_LIBRARY_PATH=/usr/lib/ollama
export LD_LIBRARY_PATH=/usr/lib/ollama:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib64:$LD_LIBRARY_PATH
export CUDA_VISIBLE_DEVICES=0

# 3. å…³é”®ï¼šç¦æ­¢ ComfyUI åå™¬æ‰€æœ‰æ˜¾å­˜
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

# 4. æ¶æ„ä¿®å¤ (å‚è€ƒä¿¡æ¯ 2)
find /usr/local/lib/python3.10/dist-packages/transformers -name "*.pyc" -delete
find /comfyui -name "*.pyc" -delete
cd /comfyui && git fetch --all && git reset --hard origin/master

# 5. å¯åŠ¨ Ollama
ollama serve > /var/log/ollama.log 2>&1 &

# 6. å¥åº·æ£€æŸ¥
python3 -c "import requests, time; 
for i in range(30):
    try:
        r = requests.get('http://127.0.0.1:11434/api/tags')
        if r.status_code == 200:
            print('âœ… Ollama å·²æ¥ç®¡ GPU'); break
    except: pass
    time.sleep(2)
"

# 7. å¯åŠ¨ ComfyUI (åŠ ä¸Šä½æ˜¾å­˜æ¨¡å¼å‚æ•°ï¼Œé˜²æ­¢æŠ¢å )
python /comfyui/main.py --listen 127.0.0.1 --port 8188 --lowvram > /var/log/comfyui.log 2>&1 &

# 8. å¯åŠ¨ä¸» Handler
python -u /comfyui/runpod_handler.py
