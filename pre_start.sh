#!/bin/bash
echo "ğŸš€ [Pre-start] æ­£åœ¨æ‰§è¡Œå…¨é‡ GPU é“¾è·¯ä¸æ¶æ„ä¿®å¤..."

# 1. è®¾ç½®è·¯å¾„ï¼ˆé”å®š Ollama å¼•æ“å¹¶ä¿ç•™ç³»ç»ŸåŸç”Ÿ CUDA è·¯å¾„ï¼‰
export OLLAMA_LIBRARY_PATH="/usr/lib/ollama"
export LD_LIBRARY_PATH="/usr/lib/ollama:/usr/local/nvidia/lib64:/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH"
export CUDA_VISIBLE_DEVICES=0

# 2. ä¾ç…§å‚è€ƒä¿¡æ¯ 2 æ‰§è¡Œæ·±åº¦æ¶æ„ä¿®å¤ï¼šæ¸…ç†ç¼“å­˜å¹¶é‡ç½® ComfyUI ä»£ç 
find /usr/local/lib/python3.10/dist-packages/transformers -name "*.pyc" -delete
find /comfyui -name "*.pyc" -delete
cd /comfyui && git fetch --all && git reset --hard origin/master

# 3. ä¿®å¤ç›®å½•ä¸æƒé™ (å‚è€ƒä¿¡æ¯ 2)
mkdir -p "/comfyui/tmp" "/comfyui/output"
chmod -R 777 "/comfyui/tmp" "/comfyui/output"
export TMPDIR="/comfyui/tmp"

# 4. å¯åŠ¨ Ollama (åå°å¹¶ç­‰å¾…åˆå§‹åŒ–)
ollama serve > /var/log/ollama.log 2>&1 &
sleep 5

# 5. å¯åŠ¨ ComfyUI ç”»å›¾åç«¯ (å‚è€ƒä¿¡æ¯ 1)
python /comfyui/main.py --listen 127.0.0.1 --port 8188 > /var/log/comfyui.log 2>&1 &

# 6. å¥åº·æ£€æŸ¥ï¼šç¡®ä¿åŒæœåŠ¡åœ¨çº¿ä¸”è¯†åˆ«ç¡¬ä»¶
python3 -c "import requests, time;
for i in range(30):
    try:
        o = requests.get('http://127.0.0.1:11434/api/tags').status_code == 200
        c = requests.get('http://127.0.0.1:8188/history').status_code == 200
        if o and c: print('âœ… Dual GPU Backends Loaded!'); break
    except: pass
    time.sleep(5)
"

# 7. å¯åŠ¨ RunPod Handler
python -u /comfyui/runpod_handler.py
