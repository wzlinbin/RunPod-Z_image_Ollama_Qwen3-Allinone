#!/bin/bash
echo "ğŸš€ [Pre-start] æ­£åœ¨åˆå§‹åŒ–åŒ GPU åŠ é€Ÿç¯å¢ƒ (Ollama + ComfyUI)..."

# 1. å¼ºåˆ¶å£°æ˜é©±åŠ¨ä¸å¼•æ“è·¯å¾„ (å¤åˆ»æ˜¨æ—¥æˆåŠŸç»éªŒ)
export OLLAMA_LIBRARY_PATH="/usr/lib/ollama"
export LD_LIBRARY_PATH="/usr/lib/ollama:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib64:$LD_LIBRARY_PATH"
export CUDA_VISIBLE_DEVICES=0

# 2. æ‰§è¡Œ ComfyUI æ¶æ„ä¿®å¤ (å¼•ç”¨å‚è€ƒä¿¡æ¯ 2)
pip install --upgrade pip --quiet
pip install --no-cache-dir transformers==4.47.0 accelerate==0.34.0 requests runpod --quiet
find /usr/local/lib/python3.10/dist-packages/transformers -name "*.pyc" -delete
find /comfyui -name "*.pyc" -delete
cd /comfyui && git fetch --all && git reset --hard origin/master

# 3. å…³é”®ï¼šå…ˆå¯åŠ¨ Ollama å¹¶ç»™å®ƒæ—¶é—´é”å®š GPU å¥æŸ„
echo "ğŸ§ª æ­£åœ¨å”¤é†’ Ollama GPU å¼•æ“..."
ollama serve > /var/log/ollama.log 2>&1 &
sleep 10  # ç»™äºˆå……è¶³æ—¶é—´è®© Ollama å®Œæˆæ˜¾å­˜æ¢æµ‹

# 4. å¯åŠ¨ ComfyUI (å¼•ç”¨å‚è€ƒä¿¡æ¯ 1)
echo "ğŸ¨ æ­£åœ¨å¯åŠ¨ ComfyUI åç«¯..."
python /comfyui/main.py --listen 127.0.0.1 --port 8188 > /var/log/comfyui.log 2>&1 &

# 5. å¥åº·æ£€æŸ¥
echo "â³ ç­‰å¾…åŒæœåŠ¡å°±ç»ª..."
python3 -c "import requests, time;
def check():
    try:
        ollama_ok = requests.get('http://127.0.0.1:11434/api/tags').status_code == 200
        comfy_ok = requests.get('http://127.0.0.1:8188/history').status_code == 200
        return ollama_ok and comfy_ok
    except: return False
for i in range(60):
    if check(): print('âœ… åŒåç«¯ GPU ç¯å¢ƒå…¨éƒ¨ Ready!'); break
    time.sleep(5)
"

# 6. å¯åŠ¨ä¸» Handler
python -u /comfyui/runpod_handler.py
