#!/bin/bash
echo "ğŸš€ [Pre-start] æ­£åœ¨åˆå§‹åŒ–åŒ GPU åŠ é€Ÿç¯å¢ƒ..."

# 1. æ ¸å¿ƒä¿®å¤ï¼šå¼ºåˆ¶å£°æ˜é©±åŠ¨ä¸å¼•æ“è·¯å¾„ (å¤åˆ»æ˜¨æ—¥æˆåŠŸç»éªŒ)
export OLLAMA_LIBRARY_PATH="/usr/lib/ollama"
export LD_LIBRARY_PATH="/usr/lib/ollama:/usr/lib/x86_64-linux-gnu:/usr/local/nvidia/lib64:$LD_LIBRARY_PATH"
export CUDA_VISIBLE_DEVICES=0

# 2. ComfyUI æ¶æ„ä¿®å¤ (å¼•ç”¨å‚è€ƒé€»è¾‘)
pip install --upgrade pip --quiet
pip install --no-cache-dir transformers==4.47.0 accelerate==0.34.0 requests runpod --quiet
find /usr/local/lib/python3.10/dist-packages/transformers -name "*.pyc" -delete
find /comfyui -name "*.pyc" -delete
cd /comfyui && git fetch --all && git reset --hard origin/master

# 3. å…³é”®ï¼šå…ˆå¯åŠ¨ Ollama å¹¶ç»™å®ƒ 5 ç§’æ—¶é—´é”å®šæ˜¾å­˜å¥æŸ„
echo "ğŸ§ª æ­£åœ¨æŠ¢å  GPU å¥æŸ„ç»™ Ollama..."
ollama serve > /var/log/ollama.log 2>&1 &
sleep 5

# 4. å¯åŠ¨ ComfyUI
echo "ğŸ¨ æ­£åœ¨å¯åŠ¨ ComfyUI..."
python /comfyui/main.py --listen 127.0.0.1 --port 8188 > /var/log/comfyui.log 2>&1 &

# 5. å¯åŠ¨ä»»åŠ¡å¤„ç†å™¨
python -u /comfyui/runpod_handler.py
